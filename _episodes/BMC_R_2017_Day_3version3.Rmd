---
title: "The Brain & Mind Centre"
author: "Sydney Informatics Hub"
subtitle: Day 1
output:
  html_document:
    hightlight: github
    theme: lumen
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
```
## Introduction to statistics with R
R provides a wide range of functions to implement statistical analysis and modelling on data. 

### Descriptive Statistics
All the standard summary statistics are available in R including:

- mean 
- sd 
- var
- min
- max
- median
- range
- quantile


```{r}
x <- c(1,3,5,2,9)
mean(x)
```
If there are missing values:
```{r}
x <- c(1,3,5,2,9,NA,7,10)
mean(x)
mean(x, na.rm=TRUE)
```

we can ignore them by setting `na.rm=TRUE`.

```{r}
x<-c(155, 160, 171, 182, 162, 153, 190, 167, 168, 165, 191)
median(x)
var(x)     # variance
sd(x)      # standard deviation
```
In R, you can use the function `quantile()` to get median and quartiles, or you can
also use the function `summary()`, to get the quantiles as well as the mean:
```{r}
# mean,median,25th and 75th quartiles,min,max
summary(x)
```
There are other functions and packages to obtain summary of statistics of the datasets:

- `sapply()`
- `pastecs()`
- `stat.desc()`
- `describe()` in the Hmisc package
- `describe()` psych package

Read in (again) the gambling dataset:
```{r}
gambling.data <- read.csv(file = "http://data.justice.qld.gov.au/JSD/OLGR/20170817_OLGR_LGA-EGM-data.csv",
                 header = TRUE,
                 sep = ",",
                 stringsAsFactors = FALSE)

# rename columns
names(gambling.data)[2] <- "Local.Govt.Area"
names(gambling.data)[7] <- "Player.Money.Lost"
names(gambling.data)[1] <- "Month.Year"

gambling.data <- na.omit(gambling.data)

#Add a day of month (1st) to each date string
date.string <- paste0( "1 " , gambling.data$Month.Year )

 #Convert to POSIXlt, a date-time format
strptime( date.string , format = "%d %B %Y" ) -> gambling.data$Date

 # subset to Brisbane only
brisbane.only <- gambling.data[gambling.data$Local.Govt.Area=="BRISBANE",]

# Choose only the brisbane data from 2010
row.indicies <- (brisbane.only$Date>="2010-01-01 AEST" &
                 brisbane.only$Date<="2010-12-31 AEST")
brisbane.2010.data <- brisbane.only[row.indicies,]
```

## Normal Distribution
A common distribution which is used in several statistical tests is a normal distribution. A standard normal distributed dataset is a dataset with variance equal to 1 and mean equal to 0, if you don't provide these arguments, then these values are assumed.

`pnorm(q)` gives the integrated probability up to `q` standard deviations away from the mean
```{r}
pnorm(q = 2)
pnorm(q = 0)
```
`qnorm(p)` gives the number of standard deviations away from the mean to get an integrated probability of `p`
```{r}
qnorm(p = 0.95)
qnorm(p = 0.025, mean = 2, sd = 0.5)
```
`rnorm(n)` gives a vector of `n` random samples from the standard normal distribution (or you can specify a mean and a standard deviation)
```{r}
rnorm(n = 4, mean = 2, sd = 2)
```
Visual ways of checking the distribution of data are histogram, boxplots, and Q-Q plot.  

## Histogram
One way to look at the distribution is to plot a histogram of the counts of the data. To obtain a histogram, the scale of the variable is divided into consecutive intervals and the number of observations in each interval
is counted.
```{r}
x <- rnorm(1000)
hist(x, freq=F)
```
Histogram with ggplot2:
```{r warning=F, echo=F}
library(ggplot2)
ggplot(data=data.frame(x=x), aes(x=x))+
  geom_histogram()+
  theme_minimal()
```

```{r}
library(ggplot2)
ggplot(data=gambling.data, aes(x=Player.Money.Lost))+
  geom_histogram()+
  scale_x_log10()+
  theme_minimal()
```

```{r}
library(ggplot2)
qplot(gambling.data$Player.Money.Lost,
      geom="histogram",
      binwidth = 1000000,  
      main = "Histogram for win", 
      xlab = "Player money lost",  
      fill=I("blue"))
```

### Boxplot
Boxplot also gives an overview of the distribution of the data. It is often used to compare data across different groups. A boxplot is a graphical representation of median and quartiles:
```{r}
boxplot(x)
```

```{r}
MyData <- gambling.data[ which(gambling.data$Local.Govt.Area %in% c("BALONNE",
                                                                    "GLADSTONE",
                                                                    "MACKAY", 
                                                                    "SCENIC RIM")) ,]
# Boxplot of Win for different LGA (selected)
boxplot(MyData$Player.Money.Lost ~ MyData$Local.Govt.Area)
```
Plot using ggplot:
```{r}
p10 <- ggplot(MyData, aes(x = Local.Govt.Area, y = Player.Money.Lost)) +
        geom_boxplot() +
        xlab("")
p10
```
###Q-Q Normal Plots
When A and B have the same distribution the Q-Q plot is a 45 degree straight line.
In many instances we need to know if a sample comes from a normal distribution, then the Q-Q plot of A against the standard normal distribution.
NOTE: This will be a straight line if the distribution of A is normal of any mean and standard deviation. 
There is an R function that does all of this: qqnorm
```{r}
v <- rnorm(100, 3, 2)
qqnorm(v)
qqline(v, col = 2)
```
The Q-Q line is drawn so that it passes through the first and third quantile. All of the points should be on this line when the sample is normal. 
```{r}
v <- rnorm(100, 0, 1)
qqnorm(v)
qqline(v, col = 2)
```
 
 
```{r}
v <- rnorm(10000, 0, 1)
qqnorm(v)
qqline(v, col = 2)

```
Q-Q Plot to Compare Two Samples
```{r}
v1 <- rnorm(100, 0, 1)
v2 <- rnorm(150, 3, 1)
qqplot(v1, v2)

```
The most common normalization is the z-transformation, where you subtract the mean and divide by the standard deviation of your variable. The result will have mean=0 and sd=1.
```{r}
v1 <- rnorm(100, 3, 5)
hist(v1)
#zVar <- (myVar - mean(myVar)) / sd(myVar)
v1_z <- ((v1 - 3)/5)
hist(v1_z)

```

##  Hypothesis testing 
A hypothesis testing can determine a cause and effect or associative relationship that is testable, e.g. weight response to deit change. The purpose and design of an experiment is to test the hypothesis. 
Firstly we set  α, arbitrary critical thresholds of probability (P-values). The P-value approach involves determining "likely" or "unlikely" by determining the probabilityby assuming the null hypothesis were true.  If the P-value is small (less than or equal to α), then we will reject the null Hypothesis in favor of the alternative hypothesis ("unlikely"); and, if the P-value is greater than α, then the null hypothesis is not rejected.
Common  α, to determine if the result are statistically significant are P<0.05, P<0.01 and P<0.001.  
Which statistical test you use depends on the type of data you have collected and the question you wish to ask. 

### hypothesis tests for population means: One-sample hypothesis test
Let x represents a sample collected from a normal population with unknown mean and standard deviation. We want to test if the population mean is equal to 9, at significance level 5%.
The R function t.test() can be used to perform both one and two sample t-tests. 
The function contains a variety of options and can be called as follows:
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95).

```{r}
x= c(6.2, 6.6, 7.1, 7.4, 7.6, 7.9, 8, 8.3, 8.4, 8.5, 8.6,
     8.8, 8.8, 9.1, 9.2, 9.4, 9.4, 9.7, 9.9, 10.2, 10.4, 10.8, 11.3, 11.9)      
# Entering the data
t.test(x-9, alternative="two.sided", conf.level=0.95)                             # Performing the t-test
```
###Interpretation of the result
The P-value (0.3622) is greater than the significance level 5% (1-0.95), so we conclude that the null hypothesis that the mean of this population is 9 is plausible.
```{r}
v <- rnorm(10,9, 6)
t.test(v-0, alternative="two.sided", conf.level=0.95)  
```

### homogeneous variances
Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisher’s F-test to verify the homoskedasticity (homogeneity of variances). In R you can do this in this way:

```{r}
a <- c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b <- c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)
var.test(a,b)
```
We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 9, and degrees of freedom of denominator = 9, using the function qf(p, df.num, df.den):

```{r}
qf(0.95, 9, 9)
```
Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.
NOTE: The F distribution has only one tail, so with a confidence level of 95%, p = 0.95. Conversely, the t-distribution has two tails, and in the R’s function qt(p, df) we insert a value p = 0975 when you’re testing a two-tailed alternative hypothesis.

### Two-sample hypothesis test
t-Test to compare the means of two groups under the assumption that both samples are random, independent, and come from normally distributed population with unknow but equal variances
To solve this problem we must use to a Student’s t-test with two samples, assuming that the two samples are taken from populations that follow a Gaussian distribution(test called Wilcoxon-Mann-Whitney test). 
For samples with Homogeneous variances (var.equal = TRUE),and independent samples (paired = FALSE: you can omit this because the function works on independent samples by default). 

```{r}
t.test(a,b, var.equal=TRUE, paired=FALSE)
```

###Interpretation of the result
The P-value (0.3348) is greater than the significance level 5% (1-0.95), so we conclude that the null hypothesis that the population means are equal is plausible.
Indeed the value of t-computed is less than the tabulated t-value for 18 degrees of freedom, which in R we can calculate:

```{r}
qt(0.975, 18)

```

This confirms that we can accept the null hypothesis H0 of equality of the means.



### Confidence interval
If we are interested in finding the confidence interval for the difference of two population means, the R-command "t.test" is also to be used.

```{r}
x=c(418,421,421,422,425,427,431,434,437,439,446,447,448,453,454,463,465)          # Entering the data into the R-workspace
y=c(429,430,430,431,36,437,440,441,445,446,447)
test2<-t.test(x,y,alternative="two.sided",mu=0,var.equal=F,conf.level=0.95)       # Performing a t-test procedure, containing a confidence interval computation
test2
```
We obtained p-value greater than 0.05, then we can conclude that the averages of two groups are significantly similar. 

### Conducting Chi-Squared Test of Independence 
The chi-square test of independence is a parametric method appropriate for testing independence between two categorical variables. 
Dataset use here is survey, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". As for Exer, they are "Freq" (frequently), "Some" and "None".
 
 
 Question: test the hypothesis whether the students smoking habit is independent of their exercise level at 0.05 significance level (explore the relationship between variabele). 
 
 First: Make a the contingency table of the two variables, the students smoking habit against the exercise level using the table function in R. The result is called the contingency table of the two variables.

```{r}
library(MASS)       # load the MASS package 
tbl <- table(survey$Smoke, survey$Exer) 
tbl                 # the contingency table

```
See the relationship using barplot:

```{r}
barplot( tbl, beside=T, legend=T)

```


Solution:We apply the chisq.test function to the contingency table tbl, and found the p-value.
```{r}
chisq.test(tbl) 

```
Answer: As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students.

Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. 
To avoid such warning, we can combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctbl instead.

```{r}
ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
ctbl 
 EX_SM<- chisq.test(ctbl) 
```

```{r}
attributes(EX_SM)
```
If the assumptions of Chi-square test are met we may considerusing Fisher's Exact test that is non-parametric alternative to the Chi-Square test. 

```{r}
fisher.test (tbl, conf.int=T, conf.level=0.99)

```

## Anova: One-Way Analysis of Variance (ANOVA)
Anova is a parameteric method appropriate for comparing the Means for the 2 or more in dependent populations. 
```{r}
attach(InsectSprays)
 data(InsectSprays)
 str(InsectSprays)

```
We’re going to use a data set called InsectSprays. 6 different insect sprays (1 Independent
Variable with 6 levels) were tested to see if there was a difference in the number of insects
found in the field after each spraying (Dependent Variable).
```{r}
InsectSprays

```

```{r}
mean(count[spray=="A"])
# Look at the mean


```


```{r}
tapply(count, spray, mean) 

```

```{r}
tapply(count, spray, var)

```

```{r}
tapply(count, spray, length)

```

```{r}
boxplot(count ~ spray)
```

Run ANOVA
H0: Mean count is the same for all sprays
```{r}
 ANOVA.Sprays <- aov(count ~ spray, data=InsectSprays)


```

```{r}
summary(ANOVA.Sprays)

```
As P-value is smaller tham 0.05 we will reject the H0; and conclude that not are the means are the same. 
```{r}
plot((ANOVA.Sprays))

```

```{r}
attributes(ANOVA.Sprays)
```

```{r}
ANOVA.Sprays$coefficients
```
Run the multiple comparison to define that which means may differ from the others.
All possible pairewise comaprison of the 
```{r}
TukeyHSD(ANOVA.Sprays)

```
Visual display of paired comparisons of the mean of insect count for sprays 
```{r}
plot(TukeyHSD(ANOVA.Sprays), las=1)
```

Kruskal Wallis One-Way Analysis of Varianve is a non-parameteric equivalent to the one-way Analysis of Variance
```{r}
kruskal.test(count ~ spray, data=InsectSprays)
```
We will reject the null hypothesis. 




### Simple Linear Regression
Simple linear regression is useful for examing or modelling the realtionship between two numeric variables.
We can also fit a simple linear regression using a categorical explanatory (X) variables. 

```{r}
 # load the package that contains the full dataset and the data viz package.
library(car)
library(ggplot2) # for some nice looking graphs!
library(MASS) # Another library for our box-cox transform down the end.

```
```{r}
# Inspect and summarize the data.
head(Prestige,5)
```


```{r}

# Subset the data to capture only income and education.
newdata = Prestige[,c(1:2)]
summary(newdata)
```
```{r}
 plot(newdata$education, newdata$income)

```

```{r}
# fit a linear model and run a summary of its results.
set.seed(1)
education.c = scale(newdata$education, center=TRUE, scale=FALSE)
mod = lm(income ~ education.c, data = newdata)
summary(mod)

```

```{r}
attributes(mod)
```

```{r}
mod$coefficients # sam as coef(mod), mod$coef

```

```{r}
confint(mod)
```

```{r} 
anova(mod)

```

```{r}
# visualize the model results.
qplot(education.c, income, data = newdata, main = "Relationship between Income and Education") +
  stat_smooth(method="lm", col="red") +
  scale_y_continuous(breaks = c(1000, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 20000, 25000), minor_breaks = NULL)

```

```{r}
# visualize residuals and fitted values.
plot(mod, pch=16, which=1)

```

##Exploratory Factor Analysis in R








