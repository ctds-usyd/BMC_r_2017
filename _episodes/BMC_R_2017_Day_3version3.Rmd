---
title: "The Brain & Mind Centre"
author: "Sydney Informatics Hub"
subtitle: Day 3
output:
  html_document:
    hightlight: github
    theme: lumen
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE)
```
### Preliminary Step: load example data

Read in (again) the gambling dataset:
```{r}
gambling.data <- read.csv(file = "http://data.justice.qld.gov.au/JSD/OLGR/20170817_OLGR_LGA-EGM-data.csv",
                 header = TRUE,
                 sep = ",",
                 stringsAsFactors = FALSE)

# rename columns
names(gambling.data)[2] <- "Local.Govt.Area"
names(gambling.data)[7] <- "Player.Money.Lost"
names(gambling.data)[1] <- "Month.Year"

gambling.data <- na.omit(gambling.data)

#Add a day of month (1st) to each date string
date.string <- paste0( "1 " , gambling.data$Month.Year )

 #Convert to POSIXlt, a date-time format
strptime( date.string , format = "%d %B %Y" ) -> gambling.data$Date

 # subset to Brisbane only
brisbane.only <- gambling.data[gambling.data$Local.Govt.Area=="BRISBANE",]

# Choose only the brisbane data from 2010
#row.indicies <- (brisbane.only$Date>="2010-01-01 AEST" &
#                 brisbane.only$Date<="2010-12-31 AEST")
#brisbane.2010.data <- brisbane.only[row.indicies,]
```

## Introduction to statistics with R
One of the really cool things about R, as opposed to other languages like python, matlab etc. is that it has such a wide user base in the statistics community and many packages that have been developed to analyze data with old and new statistical techniques and modelling. Most of these (but not all) are available on CRAN. Although we won't cover much beyond the basics today (which could just as easily be done in most other languages), more advanced things like bayesian techniques


### Descriptive Statistics

![Measures of Central Tendency](central_tendency.gif)


All the standard summary statistics are available in R including:

- mean 
- sd 
- var
- min
- max
- median
- range
- quantile


```{r}
x <- c(1,3,5,2,9,NA,7,10,15,4,7,7)
x
mean(x)
```
If there are missing values, we can ignore them by setting `na.rm=TRUE`:
```{r}
mean(x, na.rm=TRUE)
```

```{r}
median(x,na.rm = T)
var(x, na.rm = T)     # variance
sd(x, na.rm = T)      # standard deviation
```
In R, you can use the function `quantile()` to get median and quartiles, or you can
also use the function `summary()`, to get the quantiles as well as the mean, and the number of NA values:
```{r}
quantile(x, na.rm = T)

# mean,median,25th and 75th quartiles,min,max
summary(x)
```

### Visual Summaries: Box plots and Histograms

You get most of the same summary statistics visually from a boxplot: (in base R graphics)
```{r}
boxplot(x, horizontal = TRUE)
```


Or alternatively in ggplot:

```{r}
library(ggplot2)

some.LGAs<-sample(gambling.data$Local.Govt.Area,5)

subset.data<-gambling.data[gambling.data$Local.Govt.Area %in% some.LGAs,]

ggplot(data=subset.data,
       aes(x=Local.Govt.Area,y=Approved.EGMs))+
  geom_boxplot()+
  scale_y_log10()+
  coord_flip()+
  theme_minimal()

```


We can also look at the histogram of the counts of the data. To obtain a histogram, the scale of the variable is divided into consecutive intervals and the number of observations in each interval
is counted.

```{r}
hist(x)
```


Or plot a histogram in ggplot:


```{r,warning=F,message=F}
ggplot(data=subset.data,
       aes(x=Approved.EGMs,fill=Local.Govt.Area))+
  geom_histogram()+
  scale_x_log10()+
  theme_minimal()
```

### Correlation

```{r}
x1=sin(1:100/3)
y1=sin(1:100/3)
plot(x1,y1)

cor(x1,y1)


x2=rnorm(100)
y2=rnorm(100)
plot(x2,y2)

cor(x2,y2)
```

Calculating correlations for many variables at once


```{r}
#install.packages("corrplot")
library(corrplot)

MoneyDateLGA<-gambling.data[,c("Player.Money.Lost","Date","Local.Govt.Area")]

spreaddata<-tidyr::spread(MoneyDateLGA,3,1)

#remove the columns with NA values
spreaddata$TORRES<-NULL
spreaddata$LONGREACH<-NULL
spreaddata$BALONNE<-NULL

#plot corrplot
corrplot(cor(spreaddata[,-1]),
         order="hclust",
         tl.cex = .5, 
         tl.col = "black")
```


### Pair plots

```{r}
#install.packages("GGally")
library(GGally)

ggpairs(gambling.data[,-c(1,2)])
```




There are other functions and packages to obtain summary statistics of the datasets:

- `stat.desc()` in the pastecs package
- `describe()` in the Hmisc package
- `describe()` in the psych package...

## Simple Linear Regression

Simple linear regression is useful for examining or modelling the relationship between two variables.

```{r}
ggplot(data=gambling.data,
       aes(x=Operational.EGMs,
           y=Player.Money.Lost,
           color=Local.Govt.Area,
           group=1))+
  guides(color=FALSE)+
  geom_point()+
  geom_smooth(method = "lm",se=TRUE)
```


Fit a linear regression and look at a summary of its results. The model is of the form $y= mx+c$ where $y=$`Player.Money.Lost` and $x=$`Operational.EGMs`

```{r}
dollars_per_machine_model <- lm(Player.Money.Lost ~ Operational.EGMs, 
                                data = gambling.data)
summary(dollars_per_machine_model)

```

```{r}
attributes(dollars_per_machine_model)
```

```{r}
dollars_per_machine_model$coefficients 
```

```{r}
confint(dollars_per_machine_model)
```

```{r} 
anova(dollars_per_machine_model)

```


```{r}
# visualize residuals and fitted values.
plot(dollars_per_machine_model, 
     pch=16, 
     which=1)


hist(dollars_per_machine_model$residuals,
     breaks=1000,
     xlim=c(-2e6,2e6))

sd(dollars_per_machine_model$residuals)
```

### Multiple Linear Regression

We can make a much better model by including more variables. The functions we will use are exactly the same though.

```{r}
dollars_model <- lm(Player.Money.Lost ~ Operational.EGMs + Operational.Sites + as.numeric(Date) + Local.Govt.Area, 
                                data = gambling.data)
summary(dollars_model)

plot(dollars_model, 
     pch=16, 
     which=1)


hist(dollars_model$residuals,
     breaks=1000,
     xlim=c(-2e6,2e6))

sd(dollars_model$residuals)
```


## Hypothesis testing 
A hypothesis test (or more precisely a p-value from a hypothesis test) will tell you the probability that the data you have is generated by the null model (your null hypothesis).

![](redDie.jpg) 

For example, I may have rolled a red die 10 times and got the following results:

```{r}
redDie<-c(6, 4, 4, 6, 2, 1, 4, 6, 5, 2)
```

Let's say that my "null model" is that the mean of many rolls will be 3.5, i.e., the result we would expect, on average, if the die is fair.

```{r}
mean(redDie)
```

Is this data consistent with our hypothesis?

```{r}
redDieTTest<-t.test(redDie, alternative="two.sided", mu = 3.5 ,conf.level=0.95)
redDieTTest
```



```{r}
names(redDieTTest)

redDieTTest$parameter
redDieTTest$p.value

redDieTTest$conf.int
redDieTTest$estimate
redDieTTest$null.value
redDieTTest$alternative
redDieTTest
```


![](blueDie.jpg) 


I then rolled a blue die 10 times and got the following results from it:

```{r}
blueDie<-c(5, 1, 4, 2, 3, 1, 5, 4, 4, 1)
```

Are the mean values for ten roll different between the red and blue die (within statistical uncertainty) ?

```{r}
redblueDieTTest<-t.test(redDie,blueDie, alternative="two.sided", mu = 0 ,conf.level=0.95)
redblueDieTTest
```




### Interpretation of the result
The P-value (0.3622) is greater than the significance level 5% (1-0.95), so we conclude that the null hypothesis that the mean of this population is 9 is plausible.
```{r}
v <- rnorm(10,9, 6)
t.test(v-0, alternative="two.sided", conf.level=0.95)  
```

### Homogeneous variances
Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisher’s F-test to verify the homoskedasticity (homogeneity of variances). In R you can do this in this way:

```{r}
a <- c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b <- c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)
var.test(a,b)
```
We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 9, and degrees of freedom of denominator = 9, using the function qf(p, df.num, df.den):

```{r}
qf(0.95, 9, 9)
```
Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.
NOTE: The F distribution has only one tail, so with a confidence level of 95%, p = 0.95. Conversely, the t-distribution has two tails, and in the R’s function qt(p, df) we insert a value p = 0975 when you’re testing a two-tailed alternative hypothesis.

### Two-sample hypothesis test
t-Test to compare the means of two groups under the assumption that both samples are random, independent, and come from normally distributed population with unknow but equal variances
To solve this problem we must use to a Student’s t-test with two samples, assuming that the two samples are taken from populations that follow a Gaussian distribution(test called Wilcoxon-Mann-Whitney test). 
For samples with Homogeneous variances (var.equal = TRUE),and independent samples (paired = FALSE: you can omit this because the function works on independent samples by default). 

```{r}
t.test(a,b, var.equal=TRUE, paired=FALSE)
```


### Conducting Chi-Squared Test of Independence 
The chi-square test of independence is a parametric method appropriate for testing independence between two categorical variables. 
Dataset use here is survey, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". As for Exer, they are "Freq" (frequently), "Some" and "None".
 
 
 Question: test the hypothesis whether the students smoking habit is independent of their exercise level at 0.05 significance level (explore the relationship between variabele). 
 
 First: Make a the contingency table of the two variables, the students smoking habit against the exercise level using the table function in R. The result is called the contingency table of the two variables.

```{r}
library(MASS)       # load the MASS package 
tbl <- table(survey$Smoke, survey$Exer) 
tbl<-tbl[c(2,3,4,1),]                 # the contingency table
tbl
```
See the relationship using barplot:

```{r}
barplot(tbl, 
        beside=T, 
        legend=T,
        xlab = "Exercise",
        ylab = "Number of People")

```


Solution:We apply the chisq.test function to the contingency table tbl, and found the p-value.
```{r}
chisq.test(tbl) 

```
Answer: As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students.

Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. 
To avoid such warning, we can combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctbl instead.

```{r}
ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
ctbl 
 EX_SM<- chisq.test(ctbl) 
 EX_SM
```

```{r}
attributes(EX_SM)
```
If the assumptions of Chi-square test are met we may consider using Fisher's Exact test that is non-parametric alternative to the Chi-Square test. 

```{r}
fisher.test(tbl, conf.int=T, conf.level=0.99)

```

### Anova: One-Way Analysis of Variance (ANOVA)
Anova is a parameteric method appropriate for comparing the Means for the 2 or more in dependent populations. 
```{r}
attach(InsectSprays)
 data(InsectSprays)
 str(InsectSprays)

```
We’re going to use a data set called InsectSprays. 6 different insect sprays (1 Independent
Variable with 6 levels) were tested to see if there was a difference in the number of insects
found in the field after each spraying (Dependent Variable).
```{r}
InsectSprays

```

```{r}
mean(count[spray=="A"])
# Look at the mean


```


```{r}
tapply(count, spray, mean) 

```

```{r}
tapply(count, spray, var)

```

```{r}
tapply(count, spray, length)

```

```{r}
boxplot(count ~ spray)
```

Run ANOVA
H0: Mean count is the same for all sprays
```{r}
 ANOVA.Sprays <- aov(count ~ spray, data=InsectSprays)


```

```{r}
summary(ANOVA.Sprays)

```
As P-value is smaller tham 0.05 we will reject the H0; and conclude that not are the means are the same. 
```{r}
plot((ANOVA.Sprays))

```

```{r}
attributes(ANOVA.Sprays)
```

```{r}
ANOVA.Sprays$coefficients
```
Run the multiple comparison to define that which means may differ from the others.
All possible pairewise comaprison of the 
```{r}
TukeyHSD(ANOVA.Sprays)

```
Visual display of paired comparisons of the mean of insect count for sprays 
```{r}
plot(TukeyHSD(ANOVA.Sprays), las=1)
```

Kruskal Wallis One-Way Analysis of Varianve is a non-parameteric equivalent to the one-way Analysis of Variance
```{r}
kruskal.test(count ~ spray, data=InsectSprays)
```
We will reject the null hypothesis. 





## Normal Distribution
![Normal Distribution](Standard_deviation_diagram.png)

A common distribution which is used in several statistical tests is a normal distribution. A standard normal distributed dataset is a dataset with standard deviation $\sigma=1$ and mean $\mu=0$, if you don't provide these arguments, then these values are assumed.

`pnorm(q)` gives the integrated probability up to `q` standard deviations away from the mean
```{r}
pnorm(q = 2)
pnorm(q = 0)
```
`qnorm(p)` gives the number of standard deviations away from the mean to get an integrated probability of `p`
```{r}
qnorm(p = 0.95)
qnorm(p = 0.025, mean = 2, sd = 0.5)
```
`rnorm(n)` gives a vector of `n` random samples from the standard normal distribution (or you can specify a mean and a standard deviation)
```{r}
rnorm(n = 4, mean = 2, sd = 2)
```
Visual ways of checking the distribution of data are histogram, boxplots, and Q-Q plot.  


### Q-Q Normal Plots
When A and B have the same distribution the Q-Q plot is a 45 degree straight line.
In many instances we need to know if a sample comes from a normal distribution, then the Q-Q plot of A against the standard normal distribution.
NOTE: This will be a straight line if the distribution of A is normal of any mean and standard deviation. 
There is an R function that does all of this: qqnorm
```{r}
v <- rnorm(100, 3, 2)
qqnorm(v)
qqline(v, col = 2)
```
The Q-Q line is drawn so that it passes through the first and third quantile. All of the points should be on this line when the sample is normal. 
```{r}
v <- rnorm(100, 0, 1)
qqnorm(v)
qqline(v, col = 2)
```
 
 
```{r}
v <- rnorm(10000, 0, 1)
qqnorm(v)
qqline(v, col = 2)

```
Q-Q Plot to Compare Two Samples
```{r}
v1 <- rnorm(100, 0, 1)
v2 <- rnorm(150, 3, 1)
qqplot(v1, v2)

```


