---
title: "The Brain & Mind Centre"
author: "Sydney Informatics Hub"
subtitle: Day 1
output:
  html_document:
    hightlight: github
    theme: lumen
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE)
```
## Introduction to statistics with R
R provides a wide range of functions to implement statistical analysis and modelling on data. 

### Descriptive Statistics
There are wide range of function in R to obtain summary of statistics include mean, sd, var, min, max, median, range, and quantile.
```{r}
x <- c(1,3,5,2,9)
mean(x)
```
If there are missing values:
```{r}
x <- c(1,3,5,2,9,NA,7,10)
mean(x)
mean(x, na.rm=T)
```

```{r}
x<-c(155, 160, 171, 182, 162, 153, 190, 167, 168, 165, 191)
median(x)
var(x)     # variance
sd(x)      # standard deviation
```
In R, you can use function quantile to get median and quartiles, or you can
also use the function summary, to get also the mean:
```{r}
# mean,median,25th and 75th quartiles,min,max
summary(x)

```
There are other functions and packages to obtain summary of statistics of the datasets:
- sapply()
- pastecs, stat.desc
- Hmisc by using describe(x)
- psych package by describe(mydata)

Read gambling data:
```{r}
gambling.data <- read.csv(file = "http://data.justice.qld.gov.au/JSD/OLGR/20170817_OLGR_LGA-EGM-data.csv",

                 header = TRUE,

                 sep = ",",

                 stringsAsFactors = FALSE)

# rename columns

names(gambling.data)[2] <- "Local.Govt.Area"

names(gambling.data)[7] <- "Player.Money.Lost"

names(gambling.data)[1] <- "Month.Year"

gambling.data <- na.omit(gambling.data)

#Add a day of month (1st) to each date string

date.string <- paste0( "1 " , gambling.data$Month.Year )

 #Convert to POSIXlt, a date-time format

strptime( date.string , format = "%d %B %Y" ) -> gambling.data$Date

 # subset to Brisbane only

brisbane.only <- gambling.data[gambling.data$Local.Govt.Area=="BRISBANE",]

row.indicies <- (brisbane.only$Date>="2010-01-01 AEST" &

                 brisbane.only$Date<="2010-12-31 AEST")

 (brisbane.2010.data <- brisbane.only[row.indicies,])




```

## Normal Distribution
The first assumption of statistical tests, we will use here is normally distributed data. A normally distributed dataset is a dataset with variance equal to 1 and mean equal to 0.
```{r}
pnorm(2)
pnorm(0)
qnorm(0.95)
qnorm(0.025, mean = 2, sd = 0.5)
rnorm(4,2,2)
```
Visual ways of checking the distribution of data are histogram, boxplots, and Q-Q plot.  
### histogram
One way to look at the distribution, is to plot a histogram of the
data. To obtain a histogram, the scale of the variable is divided to consecutive
intervals of equal length and the number of observations in each interval
is counted.
```{r}
x <- rnorm(100)
hist(x, freq=F)
```
Histogram with ggplot2:
```{r}
 library(ggplot2)
qplot(gambling.data$Player.Money.Lost,
      geom="histogram",
      binwidth = 1000000,  
      main = "Histogram for win", 
      xlab = "Player money lost",  
      fill=I("blue"))
```
###boxplot
Boxplot also gives an overview of the distribution of the data. It is often used to compare data across different groups. A boxplot is a graphical representation of median and quartiles:
```{r}
boxplot(x)
```

```{r}
MyData <- gambling.data[ which(gambling.data$Local.Govt.Area %in% c("BALONNE", "GLADSTONE","MACKAY", "SCENIC RIM")) ,]
# Boxplot of Win for different LGA (selected)
boxplot(MyData$Player.Money.Lost ~ MyData$Local.Govt.Area)
```
Plot using ggplot:
```{r}
p10 <- ggplot(MyData, aes(x = Local.Govt.Area, y = Player.Money.Lost)) +
        geom_boxplot()
p10
```
###Q-Q Normal Plots
When A and B have the same distribution the Q-Q plot is a 45 degree straight line.
In many instances we need to know if a sample comes from a normal distribution, then the Q-Q plot of A against the standard normal distribution.
NOTE: This will be a straight line if the distribution of A is normal of any mean and standard deviation. 
There is an R function that does all of this: qqnorm
```{r}
v <- rnorm(100, 3, 2)
qqnorm(v)
qqline(v, col = 2)
```
The Q-Q line is drawn so that it passes through the first and third quantile. All of the points should be on this line when the sample is normal. 
```{r}
v <- rnorm(100, 0, 1)
qqnorm(v)
qqline(v, col = 2)
```
 
 
```{r}
v <- rnorm(10000, 0, 1)
qqnorm(v)
qqline(v, col = 2)

```
Q-Q Plot to Compare Two Samples
```{r}
v1 <- rnorm(100, 0, 1)
v2 <- rnorm(150, 3, 1)
qqplot(v1, v2)

```
The most common normalization is the z-transformation, where you subtract the mean and divide by the standard deviation of your variable. The result will have mean=0 and sd=1.
```{r}
v1 <- rnorm(100, 3, 5)
hist(v1)
#zVar <- (myVar - mean(myVar)) / sd(myVar)
v1_z <- ((v1 - 3)/5)
hist(v1_z)

```

##  Hypothesis testing 
A hypothesis testing can determine a cause and effect or associative relationship that is testable, e.g. weight response to deit change. The purpose and design of an experiment is to test the hypothesis. 
Firstly we set  α, arbitrary critical thresholds of probability (P-values). The P-value approach involves determining "likely" or "unlikely" by determining the probabilityby assuming the null hypothesis were true.  If the P-value is small (less than or equal to α), then we will reject the null Hypothesis in favor of the alternative hypothesis ("unlikely"); and, if the P-value is greater than α, then the null hypothesis is not rejected.
Common  α, to determine if the result are statistically significant are P<0.05, P<0.01 and P<0.001.  
Which statistical test you use depends on the type of data you have collected and the question you wish to ask. 

### hypothesis tests for population means: One-sample hypothesis test
Let x represents a sample collected from a normal population with unknown mean and standard deviation. We want to test if the population mean is equal to 9, at significance level 5%.
The R function t.test() can be used to perform both one and two sample t-tests. 
The function contains a variety of options and can be called as follows:
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95).

```{r}
x= c(6.2, 6.6, 7.1, 7.4, 7.6, 7.9, 8, 8.3, 8.4, 8.5, 8.6,
     8.8, 8.8, 9.1, 9.2, 9.4, 9.4, 9.7, 9.9, 10.2, 10.4, 10.8, 11.3, 11.9)      
# Entering the data
t.test(x-9, alternative="two.sided", conf.level=0.95)                             # Performing the t-test
```
###Interpretation of the result
The P-value (0.3622) is greater than the significance level 5% (1-0.95), so we conclude that the null hypothesis that the mean of this population is 9 is plausible.
```{r}
v <- rnorm(10,9, 6)
t.test(v-0, alternative="two.sided", conf.level=0.95)  
```

### homogeneous variances
Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisher’s F-test to verify the homoskedasticity (homogeneity of variances). In R you can do this in this way:

```{r}
a <- c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b <- c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)
var.test(a,b)
```
We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 9, and degrees of freedom of denominator = 9, using the function qf(p, df.num, df.den):

```{r}
qf(0.95, 9, 9)
```
Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.
NOTE: The F distribution has only one tail, so with a confidence level of 95%, p = 0.95. Conversely, the t-distribution has two tails, and in the R’s function qt(p, df) we insert a value p = 0975 when you’re testing a two-tailed alternative hypothesis.

### Two-sample hypothesis test
t-Test to compare the means of two groups under the assumption that both samples are random, independent, and come from normally distributed population with unknow but equal variances
To solve this problem we must use to a Student’s t-test with two samples, assuming that the two samples are taken from populations that follow a Gaussian distribution(test called Wilcoxon-Mann-Whitney test). 
For samples with Homogeneous variances (var.equal = TRUE),and independent samples (paired = FALSE: you can omit this because the function works on independent samples by default). 

```{r}
t.test(a,b, var.equal=TRUE, paired=FALSE)
```

###Interpretation of the result
The P-value (0.3348) is greater than the significance level 5% (1-0.95), so we conclude that the null hypothesis that the population means are equal is plausible.
Indeed the value of t-computed is less than the tabulated t-value for 18 degrees of freedom, which in R we can calculate:

```{r}
qt(0.975, 18)

```

This confirms that we can accept the null hypothesis H0 of equality of the means.



### Confidence interval
If we are interested in finding the confidence interval for the difference of two population means, the R-command "t.test" is also to be used.

```{r}
x=c(418,421,421,422,425,427,431,434,437,439,446,447,448,453,454,463,465)          # Entering the data into the R-workspace
y=c(429,430,430,431,36,437,440,441,445,446,447)
test2<-t.test(x,y,alternative="two.sided",mu=0,var.equal=F,conf.level=0.95)       # Performing a t-test procedure, containing a confidence interval computation
test2
```
We obtained p-value greater than 0.05, then we can conclude that the averages of two groups are significantly similar. 

### Chi-Squared Test
 Dataset use is survey, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". As for Exer, they are "Freq" (frequently), "Some" and "None".
Make a the contingency table of the two variables, the students smoking habit against the exercise level using the table function in R. The result is called the contingency table of the two variables.

```{r}
library(MASS)       # load the MASS package 
tbl = table(survey$Smoke, survey$Exer) 
tbl                 # the contingency table

```
Question: test the hypothesis whether the students smoking habit is independent of their exercise level at 0.05 significance level.
Solution:We apply the chisq.test function to the contingency table tbl, and found the p-value.
```{r}
chisq.test(tbl) 

```
Answer: As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students.

Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. To avoid such warning, we combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctbl instead.

```{r}
ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
ctbl 
chisq.test(ctbl) 
```
## Anova


###  Multiple (Linear) Regression


```{r}
 #
#fit <- lm(read ~ write + math + science +socst, data=hs0)
#summary(fit) # show results

```




##Exploratory Factor Analysis in R





```{r}
# Other useful functions 
#coefficients(fit) # model coefficients
#confint(fit, level=0.95) # CIs for model parameters 
#fitted(fit) # predicted values
#residuals(fit) # residuals
#anova(fit) # anova table 
#vcov(fit) # covariance matrix for model parameters 
#influence(fit) # regression diagnostics

```
Diagnostic Plots
Diagnostic plots provide checks for heteroscedasticity, normality, and influential observerations.

```{r}
# diagnostic plots 
#layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
#plot(fit)

```
Comparing Models
You can compare nested models with the anova( ) function. The following code provides a simultaneous test that x3 and x4 add to linear prediction above and beyond x1 and x2.
```{r}
# compare models
#fit1 <- lm(read ~ write + math + science +socst, data=hs0)
#fit2 <- lm(read ~ write + math , data=hs0)
#anova(fit1, fit2)

```
## more information:
### Tarnsformation to chnege a dataset to normally distributed dataset
Some measurements in nature are naturally normally distributed.  Other measurements are naturally log-normally distributed.  
Approach can be used here is to transform one or more variables to better follow a normal distribution.
For right-skewed data—tail is on the right, positive skew—, common transformations include square root, cube root, and log.
For left-skewed data—tail is on the left, negative skew—, common transformations include square root (constant – x), cube root (constant – x), and log (constant – x).
Another approach is to use a general power transformation, such as Tukey’s Ladder of Powers or a Box–Cox transformation.  These determine a lambda value, which is used as the power coefficient to transform values.  X.new = X ^ lambda for Tukey, and X.new = (X ^ lambda – 1) / lambda for Box–Cox.
The Box–Cox procedure is included in the MASS package with the function boxcox.  It uses a log-likelihood procedure to find the lambda to use to transform the dependent variable for a linear model (such as an ANOVA or linear regression).
 Useful link:
http://rcompanion.org/handbook/I_12.html
http://www.statisticssolutions.com/transforming-variables-to-meet-an-assumption/
### Hypothesis testing
 https://onlinecourses.science.psu.edu/statprogram/node/138
 https://www.r-bloggers.com/hypothesis-testing-on-normally-distributed-data-in-r/
 